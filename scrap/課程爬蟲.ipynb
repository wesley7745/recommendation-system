{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid CSRF Token\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import string\n",
    "\n",
    "def randomString(length):\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n",
    "\n",
    "\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % ''.join(randomString(8))\n",
    "XCSRF2Token = ''.join(randomString(24))\n",
    "XCSRFToken = ''.join(randomString(24))\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \"\n",
    "              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "              \"Chrome/36.0.1985.143 Safari/537.36\")\n",
    "\n",
    "post_headers = {\"User-Agent\": user_agent,\n",
    "                \"Referer\": \"https://accounts.coursera.org/signin\",\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                \"X-CSRF2-Cookie\": XCSRF2Cookie,\n",
    "                \"X-CSRF2-Token\": XCSRF2Token,\n",
    "                \"X-CSRFToken\": XCSRFToken,\n",
    "                \"Cookie\": cookie\n",
    "                }\n",
    "\n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\"email\": \"wesleyseawatch@gmail.com\",\n",
    "             \"password\": \"abc33870969\",\n",
    "             \"webrequest\": \"true\"\n",
    "             }\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \"\n",
    "              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "              \"Chrome/36.0.1985.143 Safari/537.36\")\n",
    "\n",
    "post_headers = {\"User-Agent\": user_agent,\n",
    "                \"Referer\": \"https://accounts.coursera.org/signin\"\n",
    "                }\n",
    "coursera_session = requests.Session()\n",
    "\n",
    "login_res = coursera_session.post(signin_url,\n",
    "                                  data=logininfo,\n",
    "                                  headers=post_headers,\n",
    "                                  )\n",
    "if login_res.status_code == 200:\n",
    "    print(\"Login Successfully!\")\n",
    "else:\n",
    "    print(login_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandomString\u001b[39m(length):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(random\u001b[38;5;241m.\u001b[39mchoice(string\u001b[38;5;241m.\u001b[39mletters \u001b[38;5;241m+\u001b[39m string\u001b[38;5;241m.\u001b[39mdigits) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m xrange(length))\n\u001b[1;32m----> 5\u001b[0m XCSRF2Cookie \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsrf2_token_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mrandomString\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m XCSRF2Token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(randomString(\u001b[38;5;241m24\u001b[39m))\n\u001b[0;32m      7\u001b[0m XCSRFToken \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(randomString(\u001b[38;5;241m24\u001b[39m))\n",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m, in \u001b[0;36mrandomString\u001b[1;34m(length)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandomString\u001b[39m(length):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(random\u001b[38;5;241m.\u001b[39mchoice(string\u001b[38;5;241m.\u001b[39mletters \u001b[38;5;241m+\u001b[39m string\u001b[38;5;241m.\u001b[39mdigits) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mxrange\u001b[49m(length))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "def randomString(length):\n",
    "    return ''.join(random.choice(string.letters + string.digits) for i in xrange(length))\n",
    "\n",
    "\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % ''.join(randomString(8))\n",
    "XCSRF2Token = ''.join(randomString(24))\n",
    "XCSRFToken = ''.join(randomString(24))\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "post_headers = {\"User-Agent\": user_agent,\n",
    "                \"Referer\": \"https://accounts.coursera.org/signin\",\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                \"X-CSRF2-Cookie\": XCSRF2Cookie,\n",
    "                \"X-CSRF2-Token\": XCSRF2Token,\n",
    "                \"X-CSRFToken\": XCSRFToken,\n",
    "                \"Cookie\": cookie\n",
    "                }\n",
    "\n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\"email\": \"...\",\n",
    "             \"password\": \"...\",\n",
    "             \"webrequest\": \"true\"\n",
    "             }\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \"\n",
    "              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "              \"Chrome/36.0.1985.143 Safari/537.36\")\n",
    "\n",
    "post_headers = {\"User-Agent\": user_agent,\n",
    "                \"Referer\": \"https://accounts.coursera.org/signin\"\n",
    "                }\n",
    "coursera_session = requests.Session()\n",
    "\n",
    "login_res = coursera_session.post(signin_url,\n",
    "                                  data=logininfo,\n",
    "                                  headers=post_headers,\n",
    "                                  )\n",
    "if login_res.status_code == 200:\n",
    "    print (\"Login Successfully!\")\n",
    "else:\n",
    "    print (login_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"errorCode\":\"recaptchaFailed\",\"message\":\"Recaptcha Failed\",\"details\":null}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import requests\n",
    "\n",
    "def random_string(length):\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \"\n",
    "              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "              \"Chrome/36.0.1985.143 Safari/537.36\")\n",
    "\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % random_string(8)\n",
    "XCSRF2Token = random_string(24)\n",
    "XCSRFToken = random_string(24)\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "post_headers = {\n",
    "    \"User-Agent\": user_agent,\n",
    "    \"Referer\": \"https://accounts.coursera.org/signin\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"X-CSRF2-Cookie\": XCSRF2Cookie,\n",
    "    \"X-CSRF2-Token\": XCSRF2Token,\n",
    "    \"X-CSRFToken\": XCSRFToken,\n",
    "    \"Cookie\": cookie\n",
    "}\n",
    "\n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\n",
    "    \"email\": \"wesleyseawatch@gmail.com\",\n",
    "    \"password\": \"abc33870969\",\n",
    "    \"webrequest\": \"true\"\n",
    "}\n",
    "\n",
    "coursera_session = requests.Session()\n",
    "login_res = coursera_session.post(signin_url, data=logininfo, headers=post_headers)\n",
    "\n",
    "if login_res.status_code == 200:\n",
    "    print(\"Login Successfully!\")\n",
    "else:\n",
    "    print(login_res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errorCode': 'recaptchaFailed', 'message': 'Recaptcha Failed', 'details': None}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import requests\n",
    "\n",
    "def random_string(length):\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \"\n",
    "              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "              \"Chrome/36.0.1985.143 Safari/537.36\")\n",
    "\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % random_string(8)\n",
    "XCSRF2Token = random_string(24)\n",
    "XCSRFToken = random_string(24)\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "post_headers = {\n",
    "    \"User-Agent\": user_agent,\n",
    "    \"Referer\": \"https://www.coursera.org/login\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"X-CSRF2-Cookie\": XCSRF2Cookie,\n",
    "    \"X-CSRF2-Token\": XCSRF2Token,\n",
    "    \"X-CSRFToken\": XCSRFToken,\n",
    "    \"Cookie\": cookie,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-Coursera-Application\": \"authentication\",\n",
    "    \"X-Coursera-Version\": \"4237d689319478fcb74f40349fbfc36f49345c4c\",\n",
    "    \"X-Coursera-Trace-Id-Hex\": \"205a21488310377a\"  # 使用合適的Trace ID\n",
    "}\n",
    "\n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\n",
    "    \"email\": \"wesleyseawatch@gmail.com\",\n",
    "    \"password\": \"abc33870969\",\n",
    "    \"webrequest\": \"true\"\n",
    "}\n",
    "\n",
    "coursera_session = requests.Session()\n",
    "login_res = coursera_session.post(signin_url, json=logininfo, headers=post_headers)\n",
    "\n",
    "if login_res.status_code == 200:\n",
    "    print(\"Login Successfully!\")\n",
    "else:\n",
    "    print(login_res.json())  # 打印詳細錯誤信息以進一步調試\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 400\n",
      "Response Text: Invalid CSRF Token\n",
      "Request failed with status code: 400\n",
      "Response: Invalid CSRF Token\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 將手動登錄後獲取的 cookie 放在這裡\n",
    "cookies = {\n",
    "    'usprivacy': '1---',\n",
    "    '_ga': 'GA1.1.2045465706.1720171910',\n",
    "    'OneTrustWPCCPAGoogleOptOut': 'false',\n",
    "    '_fbp': 'fb.1.1720171911603.527935714577365829',\n",
    "    'FPID': 'FPID2.2.keuy4lvZvkbc%2BGGDogv49%2FxaorVcixt%2FvViUaZM3q3I%3D.1720171910',\n",
    "    '__204u': '6600635646-1720400215294',\n",
    "    'IR_gbd': 'coursera.org',\n",
    "    'FPLC': 'vQUTzRI5iXq%2BYaDQyQU4xevxBX1PjbxBwedg19Gc45UyAW8YPZ%2FkFCOH6cCU7bHsCUViQT8N0h7gnIqQSif3hTfwu0HKL0Qmt6v%2BKaOf91HPYKfiBPPCXtxi6Oybtg%3D%3D',\n",
    "    'profileconsent': 'eyIxNDY1NDEyMjkiOnsiY2NwYVJlcXVpcmVkIjpmYWxzZSwiZ2RwclJlcXVpcmVkIjpmYWxzZX19',\n",
    "    'mutiny.user.token': '5e40ed03-1472-475b-91a4-9052f5b1d486',\n",
    "    'mutiny.user.session': '21ceec4c-88aa-4db5-8cdb-efb9564d2881',\n",
    "    'mutiny.user.session_number': '1',\n",
    "    '__400r': 'https%3A%2F%2Fwww.google.com%2F',\n",
    "    '__400v': '1bf27218-d0a6-44b3-a9a1-3bc7f76f9b54',\n",
    "    '_mkto_trk': 'id:748-MIV-116&token:_mch-coursera.org-1720405962480-97389',\n",
    "    'IR_PI': '5dddf2e5-3cd2-11ef-96d0-17223f0a4f9c%7C1720406041598',\n",
    "    'dpi_utmVals': '{\"utm_medium__c\":\"coursera\",\"utm_source__c\":\"c4b-inquiries\",\"utm_campaign__c\":\"website\",\"utm_content__c\":\"dotorg-contact-page\"}',\n",
    "    'dpi_utmOrigVals': '{\"utm_orig_medium__c\":\"coursera\",\"utm_orig_source__c\":\"c4b-inquiries\",\"utm_orig_campaign__c\":\"website\",\"utm_orig_content__c\":\"dotorg-contact-page\"}',\n",
    "    '_ga_K7L4QYTF3G': 'GS1.1.1720405956.1.1.1720406070.60.0.0',\n",
    "    'trwv.uid': 'coursera-1720405962980-b6ddcea3%3A1',\n",
    "    'trwsa.sid': 'coursera-1720405962983-3bdefc7d%3A2',\n",
    "    'fs_uid': '#ARGC0#9f9d41b0-c33a-4cfc-905e-d26681d0740f:2f31af5c-d7d7-4354-9f4d-c7caaf465759:1720405954913::2#/1751941957',\n",
    "    'maId': '{\"cid\":\"3c8278c5f3b5eda421f6de17febfd19d\",\"sid\":\"e534b718-4130-4195-9e4e-aadc4b2a25ac\",\"isSidSaved\":true,\"sessionStart\":\"2024-07-08T02:36:28.000Z\"}',\n",
    "    'CSRF3-Token': '1721270197.crVJQpr6TfmLkrgS',\n",
    "    'fs_lua': '1.1720406345336',\n",
    "    'userEmail': 'wesleyseawatch%40gmail.com',\n",
    "    '_gcl_au': '1.1.705766950.1720171910.1377519018.1720407691.1720407690',\n",
    "    'CAUTH': '6r-sLxjx_FGdRnN8XABMtvE-q0j1W44uSRkjn0qSinqbrUCcFkRPpWturphXVeWZ0jOqb2hqyJJLrJEcuF6IJA.Z4FPG_pbVj-zpFXTuRrOkw.2q0S71m2Op4kwDFzqw_RhFWciMXN1-rvMHACf8goPwLxmsrfH89fwbPNTHRiNAuSvUHI66tAp1rRzEgypTkdSbMkpKoA8oU0pQlMNzsX45b_LYjfiohHTieUXB1NXoGmikC9U2CLnVqEl3G45an2UK3IzG3mfD6kXA1lyyBfp6oIJpRCZuiVGXEiuVG_0CAknB1ZVSa6rhlbJ3Us1dgwlwbUcxmg98YgqxkoELSXmqhCgzyvnkj748JKrZXNJfoxStnEIoqPqmwQ33aq5vlZkpaNc62_x0qPwJVYtjxb6WcpGiTttsRBMEw6goDWe7iMkAm18iLH6SB4WDXDbHmCFGcL0Gyg0TRfA19tpSTru1rwyVO-V0_k-Dyjdh0qJgv3rYoKUdfMDkHy8F78zMVQBYqGeWk7u24cg1q2ygGMiWbB9qK6P-1D3QAJtDd9wSClmgjjLhh_pssDMplhjCKJ9w',\n",
    "    '_ga_7GZ59JSFWQ': 'GS1.1.1720405956.3.1.1720407694.0.0.1238666416',\n",
    "    '_ga_ZCE2Q9YZ3F': 'GS1.1.1720405956.3.1.1720407694.26.0.0',\n",
    "    '_tq_id.TV-63455409-1.39ed': '770f33746ff1a183.1720171950.0.1720407695..',\n",
    "    'IR_14726': '1720407694849%7C0%7C1720407694849%7C%7C',\n",
    "    '_uetsid': 'fff0aea03cc911ef81d4a7a06e4f923f|sa28u1|2|fna|0|1650',\n",
    "    'OptanonConsent': 'isGpcEnabled=0&datestamp=Mon+Jul+08+2024+11%3A01%3A36+GMT%2B0800+(Taipei+Standard+Time)&version=202401.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&consentId=3146ba71-bff0-43e9-9670-6dd7e937c5c1&interactionCount=0&landingPath=NotLandingPage&groups=C0001%3A1%2CC0004%3A1%2CC0002%3A1%2CC0003%3A1&AwaitingReconsent=false&geolocation=TW%3BTPE',\n",
    "    'OptanonAlertBoxClosed': '2024-07-08T03:01:36.080Z',\n",
    "    '_uetvid': '68c4d8403ab111efbcfb3fe7a69501c0|ick6q6|1720407697250|7|1|bat.bing.com/p/insights/c/s',\n",
    "    '__400vt': '1720407699974'\n",
    "}\n",
    "\n",
    "# 設置請求標頭\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\",\n",
    "    \"Referer\": \"https://www.coursera.org/\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"Content-Type\": \"application/json;charset=UTF-8\"\n",
    "}\n",
    "\n",
    "# 示例 API 請求 URL\n",
    "api_url = \"https://www.coursera.org/api/login/v3\"\n",
    "\n",
    "# 登錄信息\n",
    "logininfo = {\n",
    "    \"email\": \"wesleyseawatch@gmail.com\",\n",
    "    \"password\": \"abc33870969\",\n",
    "    \"webrequest\": \"true\"\n",
    "}\n",
    "\n",
    "# 創建一個會話\n",
    "coursera_session = requests.Session()\n",
    "\n",
    "# 設置 cookie\n",
    "for name, value in cookies.items():\n",
    "    coursera_session.cookies.set(name, value)\n",
    "\n",
    "# 發送 API 請求\n",
    "response = coursera_session.post(api_url, json=logininfo, headers=headers)\n",
    "\n",
    "# 檢查響應內容\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        json_data = response.json()\n",
    "        print(\"Login Successfully!\")\n",
    "        print(\"Response JSON:\", json_data)\n",
    "    except ValueError:\n",
    "        print(\"Response is not in JSON format\")\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "    print(\"Response:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"errorCode\":\"recaptchaFailed\",\"message\":\"Recaptcha Failed\",\"details\":null}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import requests\n",
    "\n",
    "def random_string(length):\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" \n",
    "              \"(KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\")\n",
    "\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % random_string(8)\n",
    "XCSRF2Token = random_string(24)\n",
    "XCSRFToken = random_string(24)\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "post_headers = {\n",
    "    \"User-Agent\": user_agent,\n",
    "    \"Referer\": \"https://accounts.coursera.org/signin\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"X-CSRF2-Cookie\": XCSRF2Cookie,\n",
    "    \"X-CSRF2-Token\": XCSRF2Token,\n",
    "    \"X-CSRFToken\": XCSRFToken,\n",
    "    \"Cookie\": cookie\n",
    "}\n",
    "post_headers = {\n",
    "    \"User-Agent\": user_agent,\n",
    "    \"X-Coursera-Application\" : \"authentication\",\n",
    "    \"X-Coursera-Version\": \"4237d689319478fcb74f40349fbfc36f49345c4c\", \n",
    "    \"X-Csrf2-Cookie\": csrf2_token_oNrb8mmp, \n",
    "    \"X-Csrf2-Token\": 4mGZy3eJphn9pTACNfITrU3y,\n",
    "    \"X-Csrf3-Token\": 1721270197.crVJQpr6TfmLkrgS,\n",
    "    \"X-Csrftoken\": 0JLx8HRaDuWhbOnLqslsGO6u,\n",
    "    \"X-Requested-With\": XMLHttpRequest\n",
    "}   \n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\n",
    "    \"email\": \"wesleyseawatch@gmail.com\",\n",
    "    \"password\": \"abc33870969\",\n",
    "    \"webrequest\": \"true\"\n",
    "}\n",
    "\n",
    "coursera_session = requests.Session()\n",
    "login_res = coursera_session.post(signin_url, data=logininfo, headers=post_headers)\n",
    "\n",
    "if login_res.status_code == 200:\n",
    "    print(\"Login Successfully!\")\n",
    "else:\n",
    "    print(login_res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid CSRF Token\n",
      "Status Code: 403\n",
      "Response Text: {\"errorCode\":\"Not Authorized\",\"message\":null,\"details\":null}\n",
      "Request failed with status code: 403\n",
      "Response: {\"errorCode\":\"Not Authorized\",\"message\":null,\"details\":null}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import requests\n",
    "\n",
    "def random_string(length):\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "              \"(KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\")\n",
    "\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % random_string(8)\n",
    "XCSRF2Token = random_string(24)\n",
    "XCSRFToken = random_string(24)\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "# 確保 csrf2_token_oNrb8mmp 和其他 CSRF token 值已經定義\n",
    "csrf2_token_oNrb8mmp = random_string(8)\n",
    "csrf2_token = random_string(24)\n",
    "csrf3_token = random_string(24)\n",
    "csrftoken = random_string(24)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": user_agent,\n",
    "    \"X-Coursera-Application\": \"authentication\",\n",
    "    \"X-Coursera-Version\": \"4237d689319478fcb74f40349fbfc36f49345c4c\",\n",
    "    \"X-Csrf2-Cookie\": csrf2_token_oNrb8mmp,\n",
    "    \"X-Csrf2-Token\": csrf2_token,\n",
    "    \"X-Csrf3-Token\": csrf3_token,\n",
    "    \"X-Csrftoken\": csrftoken,\n",
    "}\n",
    "\n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\n",
    "    \"email\": \"wesleyseawatch@gmail.com\",\n",
    "    \"password\": \"abc33870969\",\n",
    "    \"webrequest\": \"true\"\n",
    "}\n",
    "\n",
    "coursera_session = requests.Session()\n",
    "login_res = coursera_session.post(signin_url, json=logininfo, headers=headers)\n",
    "\n",
    "if login_res.status_code == 200:\n",
    "    print(\"Login Successfully!\")\n",
    "else:\n",
    "    print(login_res.text)\n",
    "\n",
    "# 發送後續請求\n",
    "api_url = \"https://www.coursera.org/api/adminUserPermissions.v1?q=my\"\n",
    "response = coursera_session.get(api_url, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        json_data = response.json()\n",
    "        print(\"Response JSON:\", json_data)\n",
    "    except ValueError:\n",
    "        print(\"Response is not in JSON format\")\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "    print(\"Response:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"errorCode\":\"recaptchaFailed\",\"message\":\"Recaptcha Failed\",\"details\":null}\n",
      "Status Code: 403\n",
      "Response Text: {\"errorCode\":\"Not Authorized\",\"message\":null,\"details\":null}\n",
      "Request failed with status code: 403\n",
      "Response: {\"errorCode\":\"Not Authorized\",\"message\":null,\"details\":null}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import requests\n",
    "\n",
    "def random_string(length):\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "              \"(KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\")\n",
    "\n",
    "# 確保 csrf2_token_oNrb8mmp 和其他 CSRF token 值已經定義\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % random_string(8)\n",
    "XCSRF2Token = random_string(24)\n",
    "XCSRFToken = random_string(24)\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": user_agent,\n",
    "    \"Referer\": \"https://accounts.coursera.org/signin\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"X-CSRF2-Cookie\": XCSRF2Cookie,\n",
    "    \"X-CSRF2-Token\": XCSRF2Token,\n",
    "    \"X-CSRFToken\": XCSRFToken,\n",
    "    \"Cookie\": cookie\n",
    "}\n",
    "\n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\n",
    "    \"email\": \"wesleyseawatch@gmail.com\",\n",
    "    \"password\": \"abc33870969\",\n",
    "    \"webrequest\": \"true\"\n",
    "}\n",
    "\n",
    "coursera_session = requests.Session()\n",
    "login_res = coursera_session.post(signin_url, json=logininfo, headers=headers)\n",
    "\n",
    "if login_res.status_code == 200:\n",
    "    print(\"Login Successfully!\")\n",
    "else:\n",
    "    print(login_res.text)\n",
    "\n",
    "# 發送後續請求\n",
    "api_url = \"https://www.coursera.org/api/adminUserPermissions.v1?q=my\"\n",
    "response = coursera_session.get(api_url, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        json_data = response.json()\n",
    "        print(\"Response JSON:\", json_data)\n",
    "    except ValueError:\n",
    "        print(\"Response is not in JSON format\")\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "    print(\"Response:\", response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"errorCode\":\"recaptchaFailed\",\"message\":\"Recaptcha Failed\",\"details\":null}\n",
      "Status Code: 403\n",
      "Response Text: {\"errorCode\":\"Not Authorized\",\"message\":null,\"details\":null}\n",
      "Request failed with status code: 403\n",
      "Response: {\"errorCode\":\"Not Authorized\",\"message\":null,\"details\":null}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import requests\n",
    "import six\n",
    "\n",
    "def random_string(length):\n",
    "    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in six.moves.range(length))\n",
    "\n",
    "user_agent = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "              \"(KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\")\n",
    "\n",
    "XCSRF2Cookie = 'csrf2_token_%s' % random_string(8)\n",
    "XCSRF2Token = random_string(24)\n",
    "XCSRFToken = random_string(24)\n",
    "cookie = \"csrftoken=%s; %s=%s\" % (XCSRFToken, XCSRF2Cookie, XCSRF2Token)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": user_agent,\n",
    "    \"Referer\": \"https://accounts.coursera.org/signin\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"X-CSRF2-Cookie\": XCSRF2Cookie,\n",
    "    \"X-CSRF2-Token\": XCSRF2Token,\n",
    "    \"X-CSRFToken\": XCSRFToken,\n",
    "    \"Cookie\": cookie\n",
    "}\n",
    "\n",
    "signin_url = \"https://www.coursera.org/api/login/v3\"\n",
    "logininfo = {\n",
    "    \"email\": \"wesleyseawatch@gmail.com\",\n",
    "    \"password\": \"abc33870969\",\n",
    "    \"webrequest\": \"true\"\n",
    "}\n",
    "\n",
    "coursera_session = requests.Session()\n",
    "login_res = coursera_session.post(signin_url, json=logininfo, headers=headers)\n",
    "\n",
    "if login_res.status_code == 200:\n",
    "    print(\"Login Successfully!\")\n",
    "else:\n",
    "    print(login_res.text)\n",
    "\n",
    "# 发起后续请求\n",
    "api_url = \"https://www.coursera.org/api/adminUserPermissions.v1?q=my\"\n",
    "response = coursera_session.get(api_url, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Text:\", response.text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        json_data = response.json()\n",
    "        print(\"Response JSON:\", json_data)\n",
    "    except ValueError:\n",
    "        print(\"Response is not in JSON format\")\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "    print(\"Response:\", response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: online_education\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Processing category: social_science\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Processing category: language_training\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Processing category: teacher_training\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Processing category: test_prep\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Processing category: teaching_and_academics\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Processing category: logic\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Define user inputs\n",
    "user_input = [\n",
    "    'online_education', 'social_science', 'language_training',\n",
    "    'teacher_training', 'test_prep', 'teaching_and_academics', 'logic'\n",
    "]\n",
    "\n",
    "# Initialize list to hold course data\n",
    "course_data = []\n",
    "\n",
    "# Loop through each input category\n",
    "for index in range(len(user_input)):\n",
    "    print(f\"Processing category: {user_input[index]}\")\n",
    "    page_number = 1\n",
    "    max_pages = 84\n",
    "\n",
    "    # Loop through each page for the current category\n",
    "    while page_number <= max_pages:\n",
    "        query_url = f\"https://www.coursera.org/search?query={user_input[index]}&page={page_number}&sortBy=BEST_MATCH\"\n",
    "        r = requests.get(query_url)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Page {page_number} OK\")\n",
    "            soup = bs(r.text, \"html.parser\")\n",
    "            content_boxes = soup.find_all(\"div\", class_=\"cds-ProductCard-content\")\n",
    "\n",
    "            # Loop through each course box\n",
    "            for content_box in content_boxes:\n",
    "                course_info = {\n",
    "                    'user_input': user_input[index], 'course_partner': '', 'course_title': '',\n",
    "                    'skill_gain': '', 'course_rating': '', 'course_review': '', 'course_detail': ''\n",
    "                }\n",
    "\n",
    "                # Extract course partner\n",
    "                partner_tag = content_box.find(\"p\", class_=\"cds-ProductCard-partnerNames css-vac8rf\")\n",
    "                if partner_tag:\n",
    "                    course_info['course_partner'] = partner_tag.text.strip()\n",
    "\n",
    "                # Extract course title\n",
    "                title_tag = content_box.find(\"h3\", class_=\"cds-CommonCard-title css-6ecy9b\")\n",
    "                if title_tag:\n",
    "                    course_info['course_title'] = title_tag.text.strip()\n",
    "\n",
    "                # Extract course details\n",
    "                detail_tags = content_box.find_all(\"div\", class_=\"cds-CommonCard-metadata\")\n",
    "                for detail_tag in detail_tags:\n",
    "                    detail_text = detail_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if detail_text:\n",
    "                        course_info['course_detail'] = detail_text.text.strip()\n",
    "\n",
    "                # Extract skill gain\n",
    "                skill_tag = content_box.find(\"div\", class_=\"cds-CommonCard-bodyContent\")\n",
    "                if skill_tag:\n",
    "                    skill_text = skill_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if skill_text:\n",
    "                        course_info['skill_gain'] = skill_text.text.strip()\n",
    "\n",
    "                # Extract course rating and review\n",
    "                review_tags = content_box.find_all(\"div\", class_=\"css-1vsx0as\")\n",
    "                for review_tag in review_tags:\n",
    "                    rating_tag = review_tag.find(\"p\", class_=\"css-2xargn\")\n",
    "                    if rating_tag and 'aria-label' in rating_tag.attrs:\n",
    "                        rating_text = rating_tag['aria-label']\n",
    "                        course_info['course_rating'] = rating_text.split()[0]\n",
    "                    else:\n",
    "                        course_info['course_rating'] = 'NaN'\n",
    "\n",
    "                    review_text = review_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if review_text:\n",
    "                        course_info['course_review'] = review_text.text.strip()\n",
    "                    else:\n",
    "                        course_info['course_review'] = 'NaN'\n",
    "\n",
    "        #         # Append course info to list\n",
    "        #         course_data.append(course_info)\n",
    "\n",
    "        #     # Write course data to a CSV file\n",
    "        #     csv_filename = f'course_data_{user_input[index]}.csv'\n",
    "        #     with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        #         fieldnames = ['user_input', 'course_title', 'course_partner', 'course_review', 'skill_gain', 'course_rating', 'course_detail']\n",
    "        #         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        #         if page_number == 1:\n",
    "        #             writer.writeheader()\n",
    "        #         for course_info in course_data:\n",
    "        #             writer.writerow(course_info)\n",
    "        # else:\n",
    "        #     print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "        # page_number += 1\n",
    "\n",
    "print(\"Data extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_input                                       course_title  \\\n",
      "0  online_education    Basics of Inclusive Design for Online Education   \n",
      "1  online_education  Online education: The foundations of online te...   \n",
      "2  online_education      TESOL Certificate, Part 1: Teach English Now!   \n",
      "3  online_education         Teach English Now! Foundational Principles   \n",
      "4  online_education              Online Learning  Design for Educators   \n",
      "\n",
      "                   course_partner  course_review  \\\n",
      "0  University of Colorado Boulder   (80 reviews)   \n",
      "1            Macquarie University  (212 reviews)   \n",
      "2        Arizona State University  (18K reviews)   \n",
      "3        Arizona State University  (15K reviews)   \n",
      "4            Macquarie University  (299 reviews)   \n",
      "\n",
      "                                          skill_gain  course_rating  \\\n",
      "0                                                NaN            NaN   \n",
      "1                                                NaN            NaN   \n",
      "2  Skills you'll gain: Human Learning, Planning, ...            NaN   \n",
      "3         Skills you'll gain: Mergers & Acquisitions            NaN   \n",
      "4                                                NaN            NaN   \n",
      "\n",
      "                                course_detail  \n",
      "0          Beginner Â· Course Â· 1 - 3 Months  \n",
      "1          Beginner Â· Course Â· 1 - 3 Months  \n",
      "2  Beginner Â· Specialization Â· 3 - 6 Months  \n",
      "3             Mixed Â· Course Â· 1 - 3 Months  \n",
      "4  Beginner Â· Specialization Â· 3 - 6 Months  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取爬取到的數據\n",
    "df_online_education = pd.read_csv('course_data_online_education.csv')\n",
    "df_social_science = pd.read_csv('course_data_social_science.csv')\n",
    "df_language_training = pd.read_csv('course_data_language_training.csv')\n",
    "df_teacher_training = pd.read_csv('course_data_teacher_training.csv')\n",
    "df_test_prep = pd.read_csv('course_data_test_prep.csv')\n",
    "df_teaching_and_academics = pd.read_csv('course_data_teaching_and_academics.csv')\n",
    "df_logic = pd.read_csv('course_data_logic.csv')\n",
    "\n",
    "# 查看數據的前幾行\n",
    "print(df_online_education.head())\n",
    "# print(df_social_science.head())\n",
    "# print(df_language_training.head())\n",
    "# print(df_teacher_training.head())\n",
    "# print(df_test_prep.head())\n",
    "# print(df_teaching_and_academics.head())\n",
    "# print(df_logic.head())\n",
    "\n",
    "#匯成Excel檔\n",
    "df_online_education.to_excel('course_data_online_education.xlsx', index=False)\n",
    "df_social_science.to_excel('course_data_social_science.xlsx', index=False)\n",
    "df_language_training.to_excel('course_data_language_training.xlsx', index=False)\n",
    "df_teacher_training.to_excel('course_data_teacher_training.xlsx', index=False)\n",
    "df_test_prep.to_excel('course_data_test_prep.xlsx', index=False)\n",
    "df_teaching_and_academics.to_excel('course_data_teaching_and_academics.xlsx', index=False)\n",
    "df_logic.to_excel('course_data_logic.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: data_science\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Define user inputs\n",
    "user_input = [\n",
    "    'data_science'\n",
    "]\n",
    "\n",
    "# Initialize list to hold course data\n",
    "course_data = []\n",
    "\n",
    "# Loop through each input category\n",
    "for index in range(len(user_input)):\n",
    "    print(f\"Processing category: {user_input[index]}\")\n",
    "    page_number = 1\n",
    "    max_pages = 84\n",
    "\n",
    "    # Loop through each page for the current category\n",
    "    while page_number <= max_pages:\n",
    "        query_url = f\"https://www.coursera.org/search?query={user_input[index]}&page={page_number}&sortBy=BEST_MATCH\"\n",
    "        r = requests.get(query_url)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Page {page_number} OK\")\n",
    "            soup = bs(r.text, \"html.parser\")\n",
    "            content_boxes = soup.find_all(\"div\", class_=\"cds-ProductCard-content\")\n",
    "\n",
    "            # Loop through each course box\n",
    "            for content_box in content_boxes:\n",
    "                course_info = {\n",
    "                    'user_input': user_input[index], 'course_partner': '', 'course_title': '',\n",
    "                    'skill_gain': '', 'course_rating': '', 'course_review': '', 'course_detail': ''\n",
    "                }\n",
    "\n",
    "                # Extract course partner\n",
    "                partner_tag = content_box.find(\"p\", class_=\"cds-ProductCard-partnerNames css-vac8rf\")\n",
    "                if partner_tag:\n",
    "                    course_info['course_partner'] = partner_tag.text.strip()\n",
    "\n",
    "                # Extract course title\n",
    "                title_tag = content_box.find(\"h3\", class_=\"cds-CommonCard-title css-6ecy9b\")\n",
    "                if title_tag:\n",
    "                    course_info['course_title'] = title_tag.text.strip()\n",
    "\n",
    "                # Extract course details\n",
    "                detail_tags = content_box.find_all(\"div\", class_=\"cds-CommonCard-metadata\")\n",
    "                for detail_tag in detail_tags:\n",
    "                    detail_text = detail_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if detail_text:\n",
    "                        course_info['course_detail'] = detail_text.text.strip()\n",
    "\n",
    "                # Extract skill gain\n",
    "                skill_tag = content_box.find(\"div\", class_=\"cds-CommonCard-bodyContent\")\n",
    "                if skill_tag:\n",
    "                    skill_text = skill_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if skill_text:\n",
    "                        course_info['skill_gain'] = skill_text.text.strip()\n",
    "\n",
    "                # Extract course rating and review\n",
    "                review_tags = content_box.find_all(\"div\", class_=\"css-1vsx0as\")\n",
    "                for review_tag in review_tags:\n",
    "                    rating_tag = review_tag.find(\"p\", class_=\"css-2xargn\")\n",
    "                    if rating_tag and 'aria-label' in rating_tag.attrs:\n",
    "                        rating_text = rating_tag['aria-label']\n",
    "                        course_info['course_rating'] = rating_text.split()[0]\n",
    "                    else:\n",
    "                        course_info['course_rating'] = 'NaN'\n",
    "\n",
    "                    review_text = review_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if review_text:\n",
    "                        course_info['course_review'] = review_text.text.strip()\n",
    "                    else:\n",
    "                        course_info['course_review'] = 'NaN'\n",
    "\n",
    "                # Append course info to list\n",
    "                course_data.append(course_info)\n",
    "\n",
    "            # Write course data to a CSV file\n",
    "            csv_filename = f'course_data_{user_input[index]}.csv'\n",
    "            with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['user_input', 'course_title', 'course_partner', 'course_review', 'skill_gain', 'course_rating', 'course_detail']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if page_number == 1:\n",
    "                    writer.writeheader()\n",
    "                for course_info in course_data:\n",
    "                    writer.writerow(course_info)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "print(\"Data extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_input                               course_title course_partner  \\\n",
      "0  data_science                           IBM Data Science            IBM   \n",
      "1  data_science  Python for Data Science, AI & Development            IBM   \n",
      "2  data_science                      Google Data Analytics         Google   \n",
      "3  data_science             Introduction to Data Analytics            IBM   \n",
      "4  data_science                      What is Data Science?            IBM   \n",
      "\n",
      "    course_review                                         skill_gain  \\\n",
      "0  (127K reviews)  Skills you'll gain: Python Programming, Data S...   \n",
      "1   (36K reviews)  Skills you'll gain: Computer Programming, Data...   \n",
      "2  (147K reviews)  Skills you'll gain: Data Analysis, R Programmi...   \n",
      "3   (15K reviews)  Skills you'll gain: Data Analysis, Data Manage...   \n",
      "4   (69K reviews)  Skills you'll gain: Big Data, Data Analysis, D...   \n",
      "\n",
      "   course_rating                                      course_detail  \n",
      "0            NaN  Beginner Â· Professional Certificate Â· 3 - 6 ...  \n",
      "1            NaN                 Beginner Â· Course Â· 1 - 3 Months  \n",
      "2            NaN  Beginner Â· Professional Certificate Â· 3 - 6 ...  \n",
      "3            NaN                 Beginner Â· Course Â· 1 - 3 Months  \n",
      "4            NaN                  Beginner Â· Course Â· 1 - 4 Weeks  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取爬取到的數據\n",
    "df_data_science = pd.read_csv('course_data_data_science.csv')\n",
    "\n",
    "# 查看數據的前幾行\n",
    "print(df_data_science.head())\n",
    "\n",
    "df_data_science.to_excel('course_data_data_science.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: data_science\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Define user inputs\n",
    "user_input = [\n",
    "    'data_science'\n",
    "]\n",
    "\n",
    "# Initialize list to hold course data\n",
    "course_data = []\n",
    "\n",
    "# Loop through each input category\n",
    "for index in range(len(user_input)):\n",
    "    print(f\"Processing category: {user_input[index]}\")\n",
    "    page_number = 1\n",
    "    max_pages = 84\n",
    "\n",
    "    # Loop through each page for the current category\n",
    "    while page_number <= max_pages:\n",
    "        query_url = f\"https://www.coursera.org/search?query={user_input[index]}&page={page_number}&sortBy=BEST_MATCH\"\n",
    "        r = requests.get(query_url)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Page {page_number} OK\")\n",
    "            soup = bs(r.text, \"html.parser\")\n",
    "            content_boxes = soup.find_all(\"div\", class_=\"cds-ProductCard-content\")\n",
    "\n",
    "            # Loop through each course box\n",
    "            for content_box in content_boxes:\n",
    "                course_info = {\n",
    "                    'user_input': user_input[index], 'course_partner': '', 'course_title': '',\n",
    "                    'skill_gain': '', 'course_rating': '', 'course_review': '', 'course_detail': ''\n",
    "                }\n",
    "\n",
    "                # Extract course partner\n",
    "                partner_tag = content_box.find(\"p\", class_=\"cds-ProductCard-partnerNames css-vac8rf\")\n",
    "                if partner_tag:\n",
    "                    course_info['course_partner'] = partner_tag.text.strip()\n",
    "\n",
    "                # Extract course title\n",
    "                title_tag = content_box.find(\"h3\", class_=\"cds-CommonCard-title css-6ecy9b\")\n",
    "                if title_tag:\n",
    "                    course_info['course_title'] = title_tag.text.strip()\n",
    "\n",
    "                # Extract course details\n",
    "                detail_tags = content_box.find_all(\"div\", class_=\"cds-CommonCard-metadata\")\n",
    "                for detail_tag in detail_tags:\n",
    "                    detail_text = detail_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if detail_text:\n",
    "                        course_info['course_detail'] = detail_text.text.strip()\n",
    "\n",
    "                # Extract skill gain\n",
    "                skill_tag = content_box.find(\"div\", class_=\"cds-CommonCard-bodyContent\")\n",
    "                if skill_tag:\n",
    "                    skill_text = skill_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if skill_text:\n",
    "                        course_info['skill_gain'] = skill_text.text.strip()\n",
    "\n",
    "                # Extract course rating and review\n",
    "                review_tags = content_box.find_all(\"div\", class_=\"css-1vsx0as\")\n",
    "                for review_tag in review_tags:\n",
    "                    rating_tag = review_tag.find(\"p\", {\"data-testid\": \"visually-hidden\"})\n",
    "                    if rating_tag:\n",
    "                        course_info['course_rating'] = rating_tag.get_text(strip=True)\n",
    "                    else:\n",
    "                        course_info['course_rating'] = 'NaN'\n",
    "\n",
    "                    review_text = review_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if review_text:\n",
    "                        course_info['course_review'] = review_text.text.strip()\n",
    "                    else:\n",
    "                        course_info['course_review'] = 'NaN'\n",
    "\n",
    "                # Append course info to list\n",
    "                course_data.append(course_info)\n",
    "\n",
    "            # Write course data to a CSV file\n",
    "            csv_filename = f'course_data_{user_input[index]}.csv'\n",
    "            with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['user_input', 'course_title', 'course_partner', 'course_review', 'skill_gain', 'course_rating', 'course_detail']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if page_number == 1:\n",
    "                    writer.writeheader()\n",
    "                for course_info in course_data:\n",
    "                    writer.writerow(course_info)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "print(\"Data extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最終正確版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: data_science\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Page 85 OK\n",
      "Page 86 OK\n",
      "Page 87 OK\n",
      "Page 88 OK\n",
      "Page 89 OK\n",
      "Page 90 OK\n",
      "Page 91 OK\n",
      "Page 92 OK\n",
      "Page 93 OK\n",
      "Page 94 OK\n",
      "Page 95 OK\n",
      "Page 96 OK\n",
      "Page 97 OK\n",
      "Page 98 OK\n",
      "Page 99 OK\n",
      "Page 100 OK\n",
      "Page 101 OK\n",
      "Page 102 OK\n",
      "Page 103 OK\n",
      "Page 104 OK\n",
      "Page 105 OK\n",
      "Page 106 OK\n",
      "Page 107 OK\n",
      "Page 108 OK\n",
      "Page 109 OK\n",
      "Page 110 OK\n",
      "Page 111 OK\n",
      "Page 112 OK\n",
      "Page 113 OK\n",
      "Page 114 OK\n",
      "Page 115 OK\n",
      "Page 116 OK\n",
      "Page 117 OK\n",
      "Page 118 OK\n",
      "Page 119 OK\n",
      "Page 120 OK\n",
      "Page 121 OK\n",
      "Page 122 OK\n",
      "Page 123 OK\n",
      "Page 124 OK\n",
      "Page 125 OK\n",
      "Page 126 OK\n",
      "Page 127 OK\n",
      "Page 128 OK\n",
      "Page 129 OK\n",
      "Page 130 OK\n",
      "Page 131 OK\n",
      "Page 132 OK\n",
      "Page 133 OK\n",
      "Page 134 OK\n",
      "Page 135 OK\n",
      "Page 136 OK\n",
      "Page 137 OK\n",
      "Page 138 OK\n",
      "Page 139 OK\n",
      "Page 140 OK\n",
      "Page 141 OK\n",
      "Page 142 OK\n",
      "Page 143 OK\n",
      "Page 144 OK\n",
      "Page 145 OK\n",
      "Page 146 OK\n",
      "Page 147 OK\n",
      "Page 148 OK\n",
      "Page 149 OK\n",
      "Page 150 OK\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# Define user inputs\n",
    "user_input = [\n",
    "    'data_science'\n",
    "]\n",
    "\n",
    "# Initialize list to hold course data\n",
    "course_data = []\n",
    "\n",
    "# Loop through each input category\n",
    "for index in range(len(user_input)):\n",
    "    print(f\"Processing category: {user_input[index]}\")\n",
    "    page_number = 1\n",
    "    max_pages = 150\n",
    "\n",
    "    # Loop through each page for the current category\n",
    "    while page_number <= max_pages:\n",
    "        query_url = f\"https://www.coursera.org/search?query={user_input[index]}&page={page_number}&sortBy=BEST_MATCH\"\n",
    "        r = requests.get(query_url)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Page {page_number} OK\")\n",
    "            soup = bs(r.text, \"html.parser\")\n",
    "            content_boxes = soup.find_all(\"div\", class_=\"cds-ProductCard-content\")\n",
    "\n",
    "            # Loop through each course box\n",
    "            for content_box in content_boxes:\n",
    "                course_info = {\n",
    "                    'user_input': user_input[index], 'course_partner': '', 'course_title': '',\n",
    "                    'skill_gain': '', 'course_rating': '', 'course_review': '', 'course_detail': ''\n",
    "                }\n",
    "\n",
    "                # Extract course partner\n",
    "                partner_tag = content_box.find(\"p\", class_=\"cds-ProductCard-partnerNames css-vac8rf\")\n",
    "                if partner_tag:\n",
    "                    course_info['course_partner'] = partner_tag.text.strip()\n",
    "\n",
    "                # Extract course title\n",
    "                title_tag = content_box.find(\"h3\", class_=\"cds-CommonCard-title css-6ecy9b\")\n",
    "                if title_tag:\n",
    "                    course_info['course_title'] = title_tag.text.strip()\n",
    "\n",
    "                # Extract course details\n",
    "                detail_tags = content_box.find_all(\"div\", class_=\"cds-CommonCard-metadata\")\n",
    "                details = []\n",
    "                for detail_tag in detail_tags:\n",
    "                    detail_text = detail_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if detail_text:\n",
    "                        details.append(detail_text.text.strip())\n",
    "                course_info['course_detail'] = ' '.join(details)\n",
    "\n",
    "                # Extract skill gain\n",
    "                skill_tag = content_box.find(\"div\", class_=\"cds-CommonCard-bodyContent\")\n",
    "                if skill_tag:\n",
    "                    skill_text = skill_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if skill_text:\n",
    "                        course_info['skill_gain'] = skill_text.text.strip()\n",
    "\n",
    "                # Extract course rating and review\n",
    "                rating_tag = content_box.find(\"div\", class_=\"cds-CommonCard-ratings\")\n",
    "                if rating_tag:\n",
    "                    rating_value = rating_tag.find(\"p\", class_=\"css-2xargn\")\n",
    "                    if rating_value:\n",
    "                        course_info['course_rating'] = rating_value.text.strip()\n",
    "                    else:\n",
    "                        course_info['course_rating'] = 'NaN'\n",
    "\n",
    "                    review_value = rating_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if review_value:\n",
    "                        course_info['course_review'] = review_value.text.strip()\n",
    "                    else:\n",
    "                        course_info['course_review'] = 'NaN'\n",
    "                else:\n",
    "                    course_info['course_rating'] = 'NaN'\n",
    "                    course_info['course_review'] = 'NaN'\n",
    "\n",
    "                # Append course info to list\n",
    "                course_data.append(course_info)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "# Write course data to a CSV file\n",
    "csv_filename = f'course_data_{user_input[index]}.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['user_input', 'course_title', 'course_partner', 'course_review', 'skill_gain', 'course_rating', 'course_detail']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for course_info in course_data:\n",
    "        writer.writerow(course_info)\n",
    "\n",
    "print(\"Data extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_input                               course_title  \\\n",
      "0  data_science                           IBM Data Science   \n",
      "1  data_science  Python for Data Science, AI & Development   \n",
      "2  data_science                      What is Data Science?   \n",
      "3  data_science                               Data Science   \n",
      "4  data_science                      Google Data Analytics   \n",
      "\n",
      "             course_partner   course_review  \\\n",
      "0                       IBM  (127K reviews)   \n",
      "1                       IBM   (36K reviews)   \n",
      "2                       IBM   (69K reviews)   \n",
      "3  Johns Hopkins University   (50K reviews)   \n",
      "4                    Google  (147K reviews)   \n",
      "\n",
      "                                          skill_gain  course_rating  \\\n",
      "0  Skills you'll gain: Python Programming, Data S...            4.6   \n",
      "1  Skills you'll gain: Computer Programming, Data...            4.6   \n",
      "2  Skills you'll gain: Big Data, Data Analysis, D...            4.7   \n",
      "3  Skills you'll gain: R Programming, Data Analys...            4.5   \n",
      "4  Skills you'll gain: Data Analysis, R Programmi...            4.8   \n",
      "\n",
      "                                       course_detail  \n",
      "0  Beginner Â· Professional Certificate Â· 3 - 6 ...  \n",
      "1                 Beginner Â· Course Â· 1 - 3 Months  \n",
      "2                  Beginner Â· Course Â· 1 - 4 Weeks  \n",
      "3         Beginner Â· Specialization Â· 3 - 6 Months  \n",
      "4  Beginner Â· Professional Certificate Â· 3 - 6 ...  \n",
      "     user_input                               course_title  \\\n",
      "0  data_science                           IBM Data Science   \n",
      "1  data_science  Python for Data Science, AI & Development   \n",
      "2  data_science                      What is Data Science?   \n",
      "3  data_science                               Data Science   \n",
      "4  data_science                      Google Data Analytics   \n",
      "\n",
      "             course_partner   course_review  \\\n",
      "0                       IBM  (127K reviews)   \n",
      "1                       IBM   (36K reviews)   \n",
      "2                       IBM   (69K reviews)   \n",
      "3  Johns Hopkins University   (50K reviews)   \n",
      "4                    Google  (147K reviews)   \n",
      "\n",
      "                                          skill_gain  course_rating  \\\n",
      "0  Skills you'll gain: Python Programming, Data S...            4.6   \n",
      "1  Skills you'll gain: Computer Programming, Data...            4.6   \n",
      "2  Skills you'll gain: Big Data, Data Analysis, D...            4.7   \n",
      "3  Skills you'll gain: R Programming, Data Analys...            4.5   \n",
      "4  Skills you'll gain: Data Analysis, R Programmi...            4.8   \n",
      "\n",
      "                                       course_detail  \n",
      "0  Beginner · Professional Certificate · 3 - 6 Mo...  \n",
      "1                   Beginner · Course · 1 - 3 Months  \n",
      "2                    Beginner · Course · 1 - 4 Weeks  \n",
      "3           Beginner · Specialization · 3 - 6 Months  \n",
      "4  Beginner · Professional Certificate · 3 - 6 Mo...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WCHuang8\\AppData\\Local\\Temp\\ipykernel_23100\\123221116.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_data_science = df_data_science.applymap(lambda x: x.replace('Â', '') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取爬取到的數據\n",
    "df_data_science = pd.read_csv('course_data_data_science.csv')\n",
    "\n",
    "# 查看數據的前幾行\n",
    "print(df_data_science.head())\n",
    "\n",
    "# 替換特殊字符\n",
    "df_data_science = df_data_science.applymap(lambda x: x.replace('Â', '') if isinstance(x, str) else x)\n",
    "\n",
    "# 查看清理後的數據的前幾行\n",
    "print(df_data_science.head())\n",
    "\n",
    "# 將數據保存為Excel檔案\n",
    "df_data_science.to_excel('course_data_data_science.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取所有資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: Business\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Page 85 OK\n",
      "Page 86 OK\n",
      "Page 87 OK\n",
      "Page 88 OK\n",
      "Page 89 OK\n",
      "Page 90 OK\n",
      "Page 91 OK\n",
      "Page 92 OK\n",
      "Page 93 OK\n",
      "Page 94 OK\n",
      "Page 95 OK\n",
      "Page 96 OK\n",
      "Page 97 OK\n",
      "Page 98 OK\n",
      "Page 99 OK\n",
      "Page 100 OK\n",
      "Page 101 OK\n",
      "Page 102 OK\n",
      "Page 103 OK\n",
      "Page 104 OK\n",
      "Page 105 OK\n",
      "Page 106 OK\n",
      "Page 107 OK\n",
      "Page 108 OK\n",
      "Page 109 OK\n",
      "Page 110 OK\n",
      "Page 111 OK\n",
      "Page 112 OK\n",
      "Page 113 OK\n",
      "Page 114 OK\n",
      "Page 115 OK\n",
      "Page 116 OK\n",
      "Page 117 OK\n",
      "Page 118 OK\n",
      "Page 119 OK\n",
      "Page 120 OK\n",
      "Page 121 OK\n",
      "Page 122 OK\n",
      "Page 123 OK\n",
      "Page 124 OK\n",
      "Page 125 OK\n",
      "Page 126 OK\n",
      "Page 127 OK\n",
      "Page 128 OK\n",
      "Page 129 OK\n",
      "Page 130 OK\n",
      "Page 131 OK\n",
      "Page 132 OK\n",
      "Page 133 OK\n",
      "Page 134 OK\n",
      "Page 135 OK\n",
      "Page 136 OK\n",
      "Page 137 OK\n",
      "Page 138 OK\n",
      "Page 139 OK\n",
      "Page 140 OK\n",
      "Page 141 OK\n",
      "Page 142 OK\n",
      "Page 143 OK\n",
      "Page 144 OK\n",
      "Page 145 OK\n",
      "Page 146 OK\n",
      "Page 147 OK\n",
      "Page 148 OK\n",
      "Page 149 OK\n",
      "Page 150 OK\n",
      "Page 151 OK\n",
      "Page 152 OK\n",
      "Page 153 OK\n",
      "Page 154 OK\n",
      "Page 155 OK\n",
      "Page 156 OK\n",
      "Page 157 OK\n",
      "Page 158 OK\n",
      "Page 159 OK\n",
      "Page 160 OK\n",
      "Page 161 OK\n",
      "Page 162 OK\n",
      "Page 163 OK\n",
      "Page 164 OK\n",
      "Page 165 OK\n",
      "Page 166 OK\n",
      "Page 167 OK\n",
      "Page 168 OK\n",
      "Page 169 OK\n",
      "Page 170 OK\n",
      "Page 171 OK\n",
      "Page 172 OK\n",
      "Page 173 OK\n",
      "Page 174 OK\n",
      "Page 175 OK\n",
      "Page 176 OK\n",
      "Page 177 OK\n",
      "Page 178 OK\n",
      "Page 179 OK\n",
      "Page 180 OK\n",
      "Page 181 OK\n",
      "Page 182 OK\n",
      "Page 183 OK\n",
      "Page 184 OK\n",
      "Page 185 OK\n",
      "Page 186 OK\n",
      "Page 187 OK\n",
      "Page 188 OK\n",
      "Page 189 OK\n",
      "Page 190 OK\n",
      "Page 191 OK\n",
      "Page 192 OK\n",
      "Page 193 OK\n",
      "Page 194 OK\n",
      "Page 195 OK\n",
      "Page 196 OK\n",
      "Page 197 OK\n",
      "Page 198 OK\n",
      "Page 199 OK\n",
      "Page 200 OK\n",
      "Processing category: Information Technology\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Page 85 OK\n",
      "Page 86 OK\n",
      "Page 87 OK\n",
      "Page 88 OK\n",
      "Page 89 OK\n",
      "Page 90 OK\n",
      "Page 91 OK\n",
      "Page 92 OK\n",
      "Page 93 OK\n",
      "Page 94 OK\n",
      "Page 95 OK\n",
      "Page 96 OK\n",
      "Page 97 OK\n",
      "Page 98 OK\n",
      "Page 99 OK\n",
      "Page 100 OK\n",
      "Page 101 OK\n",
      "Page 102 OK\n",
      "Page 103 OK\n",
      "Page 104 OK\n",
      "Page 105 OK\n",
      "Page 106 OK\n",
      "Page 107 OK\n",
      "Page 108 OK\n",
      "Page 109 OK\n",
      "Page 110 OK\n",
      "Page 111 OK\n",
      "Page 112 OK\n",
      "Page 113 OK\n",
      "Page 114 OK\n",
      "Page 115 OK\n",
      "Page 116 OK\n",
      "Page 117 OK\n",
      "Page 118 OK\n",
      "Page 119 OK\n",
      "Page 120 OK\n",
      "Page 121 OK\n",
      "Page 122 OK\n",
      "Page 123 OK\n",
      "Page 124 OK\n",
      "Page 125 OK\n",
      "Page 126 OK\n",
      "Page 127 OK\n",
      "Page 128 OK\n",
      "Page 129 OK\n",
      "Page 130 OK\n",
      "Page 131 OK\n",
      "Page 132 OK\n",
      "Page 133 OK\n",
      "Page 134 OK\n",
      "Page 135 OK\n",
      "Page 136 OK\n",
      "Page 137 OK\n",
      "Page 138 OK\n",
      "Page 139 OK\n",
      "Page 140 OK\n",
      "Page 141 OK\n",
      "Page 142 OK\n",
      "Page 143 OK\n",
      "Page 144 OK\n",
      "Page 145 OK\n",
      "Page 146 OK\n",
      "Page 147 OK\n",
      "Page 148 OK\n",
      "Page 149 OK\n",
      "Page 150 OK\n",
      "Page 151 OK\n",
      "Page 152 OK\n",
      "Page 153 OK\n",
      "Page 154 OK\n",
      "Page 155 OK\n",
      "Page 156 OK\n",
      "Page 157 OK\n",
      "Page 158 OK\n",
      "Page 159 OK\n",
      "Page 160 OK\n",
      "Page 161 OK\n",
      "Page 162 OK\n",
      "Page 163 OK\n",
      "Page 164 OK\n",
      "Page 165 OK\n",
      "Page 166 OK\n",
      "Page 167 OK\n",
      "Page 168 OK\n",
      "Page 169 OK\n",
      "Page 170 OK\n",
      "Page 171 OK\n",
      "Page 172 OK\n",
      "Page 173 OK\n",
      "Page 174 OK\n",
      "Page 175 OK\n",
      "Page 176 OK\n",
      "Page 177 OK\n",
      "Page 178 OK\n",
      "Page 179 OK\n",
      "Page 180 OK\n",
      "Page 181 OK\n",
      "Page 182 OK\n",
      "Page 183 OK\n",
      "Page 184 OK\n",
      "Page 185 OK\n",
      "Page 186 OK\n",
      "Page 187 OK\n",
      "Page 188 OK\n",
      "Page 189 OK\n",
      "Page 190 OK\n",
      "Page 191 OK\n",
      "Page 192 OK\n",
      "Page 193 OK\n",
      "Page 194 OK\n",
      "Page 195 OK\n",
      "Page 196 OK\n",
      "Page 197 OK\n",
      "Page 198 OK\n",
      "Page 199 OK\n",
      "Page 200 OK\n",
      "Processing category: Computer Science\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Page 85 OK\n",
      "Page 86 OK\n",
      "Page 87 OK\n",
      "Page 88 OK\n",
      "Page 89 OK\n",
      "Page 90 OK\n",
      "Page 91 OK\n",
      "Page 92 OK\n",
      "Page 93 OK\n",
      "Page 94 OK\n",
      "Page 95 OK\n",
      "Page 96 OK\n",
      "Page 97 OK\n",
      "Page 98 OK\n",
      "Page 99 OK\n",
      "Page 100 OK\n",
      "Page 101 OK\n",
      "Page 102 OK\n",
      "Page 103 OK\n",
      "Page 104 OK\n",
      "Page 105 OK\n",
      "Page 106 OK\n",
      "Page 107 OK\n",
      "Page 108 OK\n",
      "Page 109 OK\n",
      "Page 110 OK\n",
      "Page 111 OK\n",
      "Page 112 OK\n",
      "Page 113 OK\n",
      "Page 114 OK\n",
      "Page 115 OK\n",
      "Page 116 OK\n",
      "Page 117 OK\n",
      "Page 118 OK\n",
      "Page 119 OK\n",
      "Page 120 OK\n",
      "Page 121 OK\n",
      "Page 122 OK\n",
      "Page 123 OK\n",
      "Page 124 OK\n",
      "Page 125 OK\n",
      "Page 126 OK\n",
      "Page 127 OK\n",
      "Page 128 OK\n",
      "Page 129 OK\n",
      "Page 130 OK\n",
      "Page 131 OK\n",
      "Page 132 OK\n",
      "Page 133 OK\n",
      "Page 134 OK\n",
      "Page 135 OK\n",
      "Page 136 OK\n",
      "Page 137 OK\n",
      "Page 138 OK\n",
      "Page 139 OK\n",
      "Page 140 OK\n",
      "Page 141 OK\n",
      "Page 142 OK\n",
      "Page 143 OK\n",
      "Page 144 OK\n",
      "Page 145 OK\n",
      "Page 146 OK\n",
      "Page 147 OK\n",
      "Page 148 OK\n",
      "Page 149 OK\n",
      "Page 150 OK\n",
      "Page 151 OK\n",
      "Page 152 OK\n",
      "Page 153 OK\n",
      "Page 154 OK\n",
      "Page 155 OK\n",
      "Page 156 OK\n",
      "Page 157 OK\n",
      "Page 158 OK\n",
      "Page 159 OK\n",
      "Page 160 OK\n",
      "Page 161 OK\n",
      "Page 162 OK\n",
      "Page 163 OK\n",
      "Page 164 OK\n",
      "Page 165 OK\n",
      "Page 166 OK\n",
      "Page 167 OK\n",
      "Page 168 OK\n",
      "Page 169 OK\n",
      "Page 170 OK\n",
      "Page 171 OK\n",
      "Page 172 OK\n",
      "Page 173 OK\n",
      "Page 174 OK\n",
      "Page 175 OK\n",
      "Page 176 OK\n",
      "Page 177 OK\n",
      "Page 178 OK\n",
      "Page 179 OK\n",
      "Page 180 OK\n",
      "Page 181 OK\n",
      "Page 182 OK\n",
      "Page 183 OK\n",
      "Page 184 OK\n",
      "Page 185 OK\n",
      "Page 186 OK\n",
      "Page 187 OK\n",
      "Page 188 OK\n",
      "Page 189 OK\n",
      "Page 190 OK\n",
      "Page 191 OK\n",
      "Page 192 OK\n",
      "Page 193 OK\n",
      "Page 194 OK\n",
      "Page 195 OK\n",
      "Page 196 OK\n",
      "Page 197 OK\n",
      "Page 198 OK\n",
      "Page 199 OK\n",
      "Page 200 OK\n",
      "Processing category: Data Science\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Page 85 OK\n",
      "Page 86 OK\n",
      "Page 87 OK\n",
      "Page 88 OK\n",
      "Page 89 OK\n",
      "Page 90 OK\n",
      "Page 91 OK\n",
      "Page 92 OK\n",
      "Page 93 OK\n",
      "Page 94 OK\n",
      "Page 95 OK\n",
      "Page 96 OK\n",
      "Page 97 OK\n",
      "Page 98 OK\n",
      "Page 99 OK\n",
      "Page 100 OK\n",
      "Page 101 OK\n",
      "Page 102 OK\n",
      "Page 103 OK\n",
      "Page 104 OK\n",
      "Page 105 OK\n",
      "Page 106 OK\n",
      "Page 107 OK\n",
      "Page 108 OK\n",
      "Page 109 OK\n",
      "Page 110 OK\n",
      "Page 111 OK\n",
      "Page 112 OK\n",
      "Page 113 OK\n",
      "Page 114 OK\n",
      "Page 115 OK\n",
      "Page 116 OK\n",
      "Page 117 OK\n",
      "Page 118 OK\n",
      "Page 119 OK\n",
      "Page 120 OK\n",
      "Page 121 OK\n",
      "Page 122 OK\n",
      "Page 123 OK\n",
      "Page 124 OK\n",
      "Page 125 OK\n",
      "Page 126 OK\n",
      "Page 127 OK\n",
      "Page 128 OK\n",
      "Page 129 OK\n",
      "Page 130 OK\n",
      "Page 131 OK\n",
      "Page 132 OK\n",
      "Page 133 OK\n",
      "Page 134 OK\n",
      "Page 135 OK\n",
      "Page 136 OK\n",
      "Page 137 OK\n",
      "Page 138 OK\n",
      "Page 139 OK\n",
      "Page 140 OK\n",
      "Page 141 OK\n",
      "Page 142 OK\n",
      "Page 143 OK\n",
      "Page 144 OK\n",
      "Page 145 OK\n",
      "Page 146 OK\n",
      "Page 147 OK\n",
      "Page 148 OK\n",
      "Page 149 OK\n",
      "Page 150 OK\n",
      "Page 151 OK\n",
      "Page 152 OK\n",
      "Page 153 OK\n",
      "Page 154 OK\n",
      "Page 155 OK\n",
      "Page 156 OK\n",
      "Page 157 OK\n",
      "Page 158 OK\n",
      "Page 159 OK\n",
      "Page 160 OK\n",
      "Page 161 OK\n",
      "Page 162 OK\n",
      "Page 163 OK\n",
      "Page 164 OK\n",
      "Page 165 OK\n",
      "Page 166 OK\n",
      "Page 167 OK\n",
      "Page 168 OK\n",
      "Page 169 OK\n",
      "Page 170 OK\n",
      "Page 171 OK\n",
      "Page 172 OK\n",
      "Page 173 OK\n",
      "Page 174 OK\n",
      "Page 175 OK\n",
      "Page 176 OK\n",
      "Page 177 OK\n",
      "Page 178 OK\n",
      "Page 179 OK\n",
      "Page 180 OK\n",
      "Page 181 OK\n",
      "Page 182 OK\n",
      "Page 183 OK\n",
      "Page 184 OK\n",
      "Page 185 OK\n",
      "Page 186 OK\n",
      "Page 187 OK\n",
      "Page 188 OK\n",
      "Page 189 OK\n",
      "Page 190 OK\n",
      "Page 191 OK\n",
      "Page 192 OK\n",
      "Page 193 OK\n",
      "Page 194 OK\n",
      "Page 195 OK\n",
      "Page 196 OK\n",
      "Page 197 OK\n",
      "Page 198 OK\n",
      "Page 199 OK\n",
      "Page 200 OK\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define user inputs\n",
    "user_input = [\n",
    "    'Business',\n",
    "    'Information Technology',\n",
    "    'Computer Science',\n",
    "    'Data Science',\n",
    "]\n",
    "\n",
    "# Initialize list to hold course data\n",
    "course_data = []\n",
    "\n",
    "# Loop through each input category\n",
    "for index in range(len(user_input)):\n",
    "    print(f\"Processing category: {user_input[index]}\")\n",
    "    page_number = 1\n",
    "    max_pages = 200\n",
    "\n",
    "    # Loop through each page for the current category\n",
    "    while page_number <= max_pages:\n",
    "        query_url = f\"https://www.coursera.org/search?query={user_input[index]}&page={page_number}&sortBy=BEST_MATCH\"\n",
    "        r = requests.get(query_url)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Page {page_number} OK\")\n",
    "            soup = bs(r.text, \"html.parser\")\n",
    "            content_boxes = soup.find_all(\"div\", class_=\"cds-ProductCard-content\")\n",
    "\n",
    "            # Loop through each course box\n",
    "            for content_box in content_boxes:\n",
    "                course_info = {\n",
    "                    'user_input': user_input[index], 'course_partner': '', 'course_title': '',\n",
    "                    'skill_gain': '', 'course_rating': '', 'course_review': '', 'course_detail': ''\n",
    "                }\n",
    "\n",
    "                # Extract course partner\n",
    "                partner_tag = content_box.find(\"p\", class_=\"cds-ProductCard-partnerNames css-vac8rf\")\n",
    "                if partner_tag:\n",
    "                    course_info['course_partner'] = partner_tag.text.strip()\n",
    "\n",
    "                # Extract course title\n",
    "                title_tag = content_box.find(\"h3\", class_=\"cds-CommonCard-title css-6ecy9b\")\n",
    "                if title_tag:\n",
    "                    course_info['course_title'] = title_tag.text.strip()\n",
    "\n",
    "                # Extract course details\n",
    "                detail_tags = content_box.find_all(\"div\", class_=\"cds-CommonCard-metadata\")\n",
    "                details = []\n",
    "                for detail_tag in detail_tags:\n",
    "                    detail_text = detail_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if detail_text:\n",
    "                        details.append(detail_text.text.strip())\n",
    "                course_info['course_detail'] = ' '.join(details)\n",
    "\n",
    "                # Extract skill gain\n",
    "                skill_tag = content_box.find(\"div\", class_=\"cds-CommonCard-bodyContent\")\n",
    "                if skill_tag:\n",
    "                    skill_text = skill_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if skill_text:\n",
    "                        course_info['skill_gain'] = skill_text.text.strip()\n",
    "\n",
    "                # Extract course rating and review\n",
    "                rating_tag = content_box.find(\"div\", class_=\"cds-CommonCard-ratings\")\n",
    "                if rating_tag:\n",
    "                    rating_value = rating_tag.find(\"p\", class_=\"css-2xargn\")\n",
    "                    if rating_value:\n",
    "                        course_info['course_rating'] = rating_value.text.strip()\n",
    "                    else:\n",
    "                        course_info['course_rating'] = 'NaN'\n",
    "\n",
    "                    review_value = rating_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                    if review_value:\n",
    "                        course_info['course_review'] = review_value.text.strip()\n",
    "                    else:\n",
    "                        course_info['course_review'] = 'NaN'\n",
    "                else:\n",
    "                    course_info['course_rating'] = 'NaN'\n",
    "                    course_info['course_review'] = 'NaN'\n",
    "\n",
    "                # Append course info to list\n",
    "                course_data.append(course_info)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "# Create directory if it does not exist\n",
    "output_directory = r'C:\\Users\\WCHuang8\\Desktop\\學習推薦系統專案\\coursera爬蟲\\所有課程資訊'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Write course data to a CSV file\n",
    "csv_filename = os.path.join(output_directory, f'course_data_all.csv')\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['user_input', 'course_title', 'course_partner', 'course_review', 'skill_gain', 'course_rating', 'course_detail']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for course_info in course_data:\n",
    "        writer.writerow(course_info)\n",
    "\n",
    "print(\"Data extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_input                            course_title  \\\n",
      "0   Business  Business Analysis & Process Management   \n",
      "1   Business                       Financial Markets   \n",
      "2   Business                    Business Foundations   \n",
      "3   Business             Launch Your Online Business   \n",
      "4   Business                        Entrepreneurship   \n",
      "\n",
      "                     course_partner   course_review  \\\n",
      "0          Coursera Project Network  (3.7K reviews)   \n",
      "1                   Yale University   (27K reviews)   \n",
      "2        University of Pennsylvania   (25K reviews)   \n",
      "3  The State University of New York   (599 reviews)   \n",
      "4        University of Pennsylvania  (5.4K reviews)   \n",
      "\n",
      "                                          skill_gain  course_rating  \\\n",
      "0  Skills you'll gain: Business Analysis, Busines...            4.4   \n",
      "1  Skills you'll gain: Finance, Risk Management, ...            4.8   \n",
      "2  Skills you'll gain: Accounting, Finance, Gener...            4.7   \n",
      "3  Skills you'll gain: Strategy, Brand Management...            4.7   \n",
      "4  Skills you'll gain: Entrepreneurship, Strategy...            4.8   \n",
      "\n",
      "                                     course_detail  \n",
      "0  Beginner Â· Guided Project Â· Less Than 2 Hours  \n",
      "1               Beginner Â· Course Â· 1 - 3 Months  \n",
      "2       Beginner Â· Specialization Â· 3 - 6 Months  \n",
      "3                Beginner Â· Course Â· 1 - 4 Weeks  \n",
      "4       Beginner Â· Specialization Â· 3 - 6 Months  \n",
      "  user_input                            course_title  \\\n",
      "0   Business  Business Analysis & Process Management   \n",
      "1   Business                       Financial Markets   \n",
      "2   Business                    Business Foundations   \n",
      "3   Business             Launch Your Online Business   \n",
      "4   Business                        Entrepreneurship   \n",
      "\n",
      "                     course_partner   course_review  \\\n",
      "0          Coursera Project Network  (3.7K reviews)   \n",
      "1                   Yale University   (27K reviews)   \n",
      "2        University of Pennsylvania   (25K reviews)   \n",
      "3  The State University of New York   (599 reviews)   \n",
      "4        University of Pennsylvania  (5.4K reviews)   \n",
      "\n",
      "                                          skill_gain  course_rating  \\\n",
      "0  Skills you'll gain: Business Analysis, Busines...            4.4   \n",
      "1  Skills you'll gain: Finance, Risk Management, ...            4.8   \n",
      "2  Skills you'll gain: Accounting, Finance, Gener...            4.7   \n",
      "3  Skills you'll gain: Strategy, Brand Management...            4.7   \n",
      "4  Skills you'll gain: Entrepreneurship, Strategy...            4.8   \n",
      "\n",
      "                                   course_detail  \n",
      "0  Beginner · Guided Project · Less Than 2 Hours  \n",
      "1               Beginner · Course · 1 - 3 Months  \n",
      "2       Beginner · Specialization · 3 - 6 Months  \n",
      "3                Beginner · Course · 1 - 4 Weeks  \n",
      "4       Beginner · Specialization · 3 - 6 Months  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WCHuang8\\AppData\\Local\\Temp\\ipykernel_10552\\3522099728.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_data_science = df_data_science.applymap(lambda x: x.replace('Â', '') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取爬取到的數據\n",
    "df_data_science = pd.read_csv(\"C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_data_all.csv\")\n",
    "\n",
    "# 查看數據的前幾行\n",
    "print(df_data_science.head())\n",
    "\n",
    "# 替換特殊字符\n",
    "df_data_science = df_data_science.applymap(lambda x: x.replace('Â', '') if isinstance(x, str) else x)\n",
    "\n",
    "# 查看清理後的數據的前幾行\n",
    "print(df_data_science.head())\n",
    "\n",
    "# 將數據保存為Excel檔案\n",
    "df_data_science.to_excel('C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_data_all.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_input                                       course_title  \\\n",
      "0          Business             Business Analysis & Process Management   \n",
      "1          Business                                  Financial Markets   \n",
      "2          Business                               Business Foundations   \n",
      "3          Business                        Launch Your Online Business   \n",
      "4          Business                                   Entrepreneurship   \n",
      "...             ...                                                ...   \n",
      "14289  Data Science                           Introduction to Node-red   \n",
      "14290  Data Science  Dimensionality Reduction using an Autoencoder ...   \n",
      "14291  Data Science  Using Basic Formulas and Functions in Microsof...   \n",
      "14292  Data Science                     Fundamentals of  CNNs and RNNs   \n",
      "14294  Data Science                         Introductory C Programming   \n",
      "\n",
      "                         course_partner   course_review  \\\n",
      "0              Coursera Project Network  (3.7K reviews)   \n",
      "1                       Yale University   (27K reviews)   \n",
      "2            University of Pennsylvania   (25K reviews)   \n",
      "3      The State University of New York   (599 reviews)   \n",
      "4            University of Pennsylvania  (5.4K reviews)   \n",
      "...                                 ...             ...   \n",
      "14289          Coursera Project Network    (38 reviews)   \n",
      "14290          Coursera Project Network    (99 reviews)   \n",
      "14291          Coursera Project Network  (1.6K reviews)   \n",
      "14292           Sungkyunkwan University    (22 reviews)   \n",
      "14294                   Duke University  (7.2K reviews)   \n",
      "\n",
      "                                              skill_gain  course_rating  \\\n",
      "0      Skills you'll gain: Business Analysis, Busines...            4.4   \n",
      "1      Skills you'll gain: Finance, Risk Management, ...            4.8   \n",
      "2      Skills you'll gain: Accounting, Finance, Gener...            4.7   \n",
      "3      Skills you'll gain: Strategy, Brand Management...            4.7   \n",
      "4      Skills you'll gain: Entrepreneurship, Strategy...            4.8   \n",
      "...                                                  ...            ...   \n",
      "14289                     Skills you'll gain: Javascript            4.8   \n",
      "14290  Skills you'll gain: Dimensionality Reduction, ...            4.6   \n",
      "14291  Skills you'll gain: Data Analysis, Microsoft E...            4.6   \n",
      "14292  Skills you'll gain: Artificial Neural Networks...            4.1   \n",
      "14294  Skills you'll gain: Python Programming, Comput...            4.6   \n",
      "\n",
      "                                           course_detail  contains_ai  \\\n",
      "0          Beginner · Guided Project · Less Than 2 Hours         True   \n",
      "1                       Beginner · Course · 1 - 3 Months         True   \n",
      "2               Beginner · Specialization · 3 - 6 Months         True   \n",
      "3                        Beginner · Course · 1 - 4 Weeks         True   \n",
      "4               Beginner · Specialization · 3 - 6 Months         True   \n",
      "...                                                  ...          ...   \n",
      "14289      Beginner · Guided Project · Less Than 2 Hours         True   \n",
      "14290  Intermediate · Guided Project · Less Than 2 Hours         True   \n",
      "14291      Beginner · Guided Project · Less Than 2 Hours         True   \n",
      "14292                   Beginner · Course · 1 - 3 Months         True   \n",
      "14294           Beginner · Specialization · 3 - 6 Months         True   \n",
      "\n",
      "                                             ai_keywords  \n",
      "0                                                   , AI  \n",
      "1                                                   , AI  \n",
      "2                                                   , AI  \n",
      "3                                                   , AI  \n",
      "4                                                   , AI  \n",
      "...                                                  ...  \n",
      "14289                                               , AI  \n",
      "14290                             , AI, Machine Learning  \n",
      "14291                                               , AI  \n",
      "14292  , AI, Machine Learning, Deep Learning, Neural ...  \n",
      "14294                                               , AI  \n",
      "\n",
      "[3404 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取Excel文件\n",
    "file_path = 'C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_data_all.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 删除包含缺漏值（NaN）的行\n",
    "df = df.dropna()\n",
    "\n",
    "# 定義AI相關的關鍵詞\n",
    "ai_keywords = [\n",
    "    'AI', 'Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Neural Network',\n",
    "     'Artificial Neural Networks', 'NLP', 'Natural Language Processing',\n",
    "    'Computer Vision', 'Robotics'\n",
    "]\n",
    "\n",
    "# 判斷是否包含AI關鍵詞的函數\n",
    "def contains_ai_keywords(text):\n",
    "    if isinstance(text, str):  # 檢查是否為字符串\n",
    "        for keyword in ai_keywords:\n",
    "            if keyword.lower() in text.lower():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# 增加一列顯示具體包含的AI關鍵詞\n",
    "def extract_ai_keywords(text):\n",
    "    found_keywords = []\n",
    "    if isinstance(text, str):\n",
    "        for keyword in ai_keywords:\n",
    "            if keyword.lower() in text.lower():\n",
    "                found_keywords.append(keyword)\n",
    "    return ', '.join(found_keywords)\n",
    "\n",
    "# 篩選包含AI關鍵詞的課程\n",
    "df['contains_ai'] = df.apply(lambda row: contains_ai_keywords(row['course_title']) or contains_ai_keywords(row['skill_gain']), axis=1)\n",
    "df['ai_keywords'] = df.apply(lambda row: extract_ai_keywords(row['course_title']) + ', ' + extract_ai_keywords(row['skill_gain']), axis=1)\n",
    "filtered_df = df[df['contains_ai']]\n",
    "\n",
    "# 輸出包含AI關鍵詞的課程\n",
    "print(filtered_df)\n",
    "\n",
    "# 如果需要將結果保存為新的Excel文件\n",
    "output_file_path = 'C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_ai_all_revised.xlsx'\n",
    "filtered_df.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: AI\n",
      "Page 1 OK\n",
      "Page 2 OK\n",
      "Page 3 OK\n",
      "Page 4 OK\n",
      "Page 5 OK\n",
      "Page 6 OK\n",
      "Page 7 OK\n",
      "Page 8 OK\n",
      "Page 9 OK\n",
      "Page 10 OK\n",
      "Page 11 OK\n",
      "Page 12 OK\n",
      "Page 13 OK\n",
      "Page 14 OK\n",
      "Page 15 OK\n",
      "Page 16 OK\n",
      "Page 17 OK\n",
      "Page 18 OK\n",
      "Page 19 OK\n",
      "Page 20 OK\n",
      "Page 21 OK\n",
      "Page 22 OK\n",
      "Page 23 OK\n",
      "Page 24 OK\n",
      "Page 25 OK\n",
      "Page 26 OK\n",
      "Page 27 OK\n",
      "Page 28 OK\n",
      "Page 29 OK\n",
      "Page 30 OK\n",
      "Page 31 OK\n",
      "Page 32 OK\n",
      "Page 33 OK\n",
      "Page 34 OK\n",
      "Page 35 OK\n",
      "Page 36 OK\n",
      "Page 37 OK\n",
      "Page 38 OK\n",
      "Page 39 OK\n",
      "Page 40 OK\n",
      "Page 41 OK\n",
      "Page 42 OK\n",
      "Page 43 OK\n",
      "Page 44 OK\n",
      "Page 45 OK\n",
      "Page 46 OK\n",
      "Page 47 OK\n",
      "Page 48 OK\n",
      "Page 49 OK\n",
      "Page 50 OK\n",
      "Page 51 OK\n",
      "Page 52 OK\n",
      "Page 53 OK\n",
      "Page 54 OK\n",
      "Page 55 OK\n",
      "Page 56 OK\n",
      "Page 57 OK\n",
      "Page 58 OK\n",
      "Page 59 OK\n",
      "Page 60 OK\n",
      "Page 61 OK\n",
      "Page 62 OK\n",
      "Page 63 OK\n",
      "Page 64 OK\n",
      "Page 65 OK\n",
      "Page 66 OK\n",
      "Page 67 OK\n",
      "Page 68 OK\n",
      "Page 69 OK\n",
      "Page 70 OK\n",
      "Page 71 OK\n",
      "Page 72 OK\n",
      "Page 73 OK\n",
      "Page 74 OK\n",
      "Page 75 OK\n",
      "Page 76 OK\n",
      "Page 77 OK\n",
      "Page 78 OK\n",
      "Page 79 OK\n",
      "Page 80 OK\n",
      "Page 81 OK\n",
      "Page 82 OK\n",
      "Page 83 OK\n",
      "Page 84 OK\n",
      "Page 85 OK\n",
      "Page 86 OK\n",
      "Page 87 OK\n",
      "Page 88 OK\n",
      "Page 89 OK\n",
      "Page 90 OK\n",
      "Page 91 OK\n",
      "Page 92 OK\n",
      "Page 93 OK\n",
      "Page 94 OK\n",
      "Page 95 OK\n",
      "Page 96 OK\n",
      "Page 97 OK\n",
      "Page 98 OK\n",
      "Page 99 OK\n",
      "Page 100 OK\n",
      "Page 101 OK\n",
      "Page 102 OK\n",
      "Page 103 OK\n",
      "Page 104 OK\n",
      "Page 105 OK\n",
      "Page 106 OK\n",
      "Page 107 OK\n",
      "Page 108 OK\n",
      "Page 109 OK\n",
      "Page 110 OK\n",
      "Page 111 OK\n",
      "Page 112 OK\n",
      "Page 113 OK\n",
      "Page 114 OK\n",
      "Page 115 OK\n",
      "Page 116 OK\n",
      "Page 117 OK\n",
      "Page 118 OK\n",
      "Page 119 OK\n",
      "Page 120 OK\n",
      "Page 121 OK\n",
      "Page 122 OK\n",
      "Page 123 OK\n",
      "Page 124 OK\n",
      "Page 125 OK\n",
      "Page 126 OK\n",
      "Page 127 OK\n",
      "Page 128 OK\n",
      "Page 129 OK\n",
      "Page 130 OK\n",
      "Page 131 OK\n",
      "Page 132 OK\n",
      "Page 133 OK\n",
      "Page 134 OK\n",
      "Page 135 OK\n",
      "Page 136 OK\n",
      "Page 137 OK\n",
      "Page 138 OK\n",
      "Page 139 OK\n",
      "Page 140 OK\n",
      "Page 141 OK\n",
      "Page 142 OK\n",
      "Page 143 OK\n",
      "Page 144 OK\n",
      "Page 145 OK\n",
      "Page 146 OK\n",
      "Page 147 OK\n",
      "Page 148 OK\n",
      "Page 149 OK\n",
      "Page 150 OK\n",
      "Page 151 OK\n",
      "Page 152 OK\n",
      "Page 153 OK\n",
      "Page 154 OK\n",
      "Page 155 OK\n",
      "Page 156 OK\n",
      "Page 157 OK\n",
      "Page 158 OK\n",
      "Page 159 OK\n",
      "Page 160 OK\n",
      "Page 161 OK\n",
      "Page 162 OK\n",
      "Page 163 OK\n",
      "Page 164 OK\n",
      "Page 165 OK\n",
      "Page 166 OK\n",
      "Page 167 OK\n",
      "Page 168 OK\n",
      "Page 169 OK\n",
      "Page 170 OK\n",
      "Page 171 OK\n",
      "Page 172 OK\n",
      "Page 173 OK\n",
      "Page 174 OK\n",
      "Page 175 OK\n",
      "Page 176 OK\n",
      "Page 177 OK\n",
      "Page 178 OK\n",
      "Page 179 OK\n",
      "Page 180 OK\n",
      "Page 181 OK\n",
      "Page 182 OK\n",
      "Page 183 OK\n",
      "Page 184 OK\n",
      "Page 185 OK\n",
      "Page 186 OK\n",
      "Page 187 OK\n",
      "Page 188 OK\n",
      "Page 189 OK\n",
      "Page 190 OK\n",
      "Page 191 OK\n",
      "Page 192 OK\n",
      "Page 193 OK\n",
      "Page 194 OK\n",
      "Page 195 OK\n",
      "Page 196 OK\n",
      "Page 197 OK\n",
      "Page 198 OK\n",
      "Page 199 OK\n",
      "Page 200 OK\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define user input\n",
    "user_input = 'AI'\n",
    "\n",
    "# Initialize list to hold course data\n",
    "course_data = []\n",
    "\n",
    "print(f\"Processing category: {user_input}\")\n",
    "page_number = 1\n",
    "max_pages = 200\n",
    "\n",
    "# Loop through each page for the current category\n",
    "while page_number <= max_pages:\n",
    "    query_url = f\"https://www.coursera.org/search?query={user_input}&page={page_number}&sortBy=BEST_MATCH\"\n",
    "    r = requests.get(query_url)\n",
    "    if r.status_code == 200:\n",
    "        print(f\"Page {page_number} OK\")\n",
    "        soup = bs(r.text, \"html.parser\")\n",
    "        content_boxes = soup.find_all(\"div\", class_=\"cds-ProductCard-content\")\n",
    "\n",
    "        # Loop through each course box\n",
    "        for content_box in content_boxes:\n",
    "            course_info = {\n",
    "                'user_input': user_input, 'course_partner': '', 'course_title': '',\n",
    "                'skill_gain': '', 'course_rating': '', 'course_review': '', 'course_detail': ''\n",
    "            }\n",
    "\n",
    "            # Extract course partner\n",
    "            partner_tag = content_box.find(\"p\", class_=\"cds-ProductCard-partnerNames css-vac8rf\")\n",
    "            if partner_tag:\n",
    "                course_info['course_partner'] = partner_tag.text.strip()\n",
    "\n",
    "            # Extract course title\n",
    "            title_tag = content_box.find(\"h3\", class_=\"cds-CommonCard-title css-6ecy9b\")\n",
    "            if title_tag:\n",
    "                course_info['course_title'] = title_tag.text.strip()\n",
    "\n",
    "            # Extract course details\n",
    "            detail_tags = content_box.find_all(\"div\", class_=\"cds-CommonCard-metadata\")\n",
    "            details = []\n",
    "            for detail_tag in detail_tags:\n",
    "                detail_text = detail_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                if detail_text:\n",
    "                    details.append(detail_text.text.strip())\n",
    "            course_info['course_detail'] = ' '.join(details)\n",
    "\n",
    "            # Extract skill gain\n",
    "            skill_tag = content_box.find(\"div\", class_=\"cds-CommonCard-bodyContent\")\n",
    "            if skill_tag:\n",
    "                skill_text = skill_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                if skill_text:\n",
    "                    course_info['skill_gain'] = skill_text.text.strip()\n",
    "\n",
    "            # Extract course rating and review\n",
    "            rating_tag = content_box.find(\"div\", class_=\"cds-CommonCard-ratings\")\n",
    "            if rating_tag:\n",
    "                rating_value = rating_tag.find(\"p\", class_=\"css-2xargn\")\n",
    "                if rating_value:\n",
    "                    course_info['course_rating'] = rating_value.text.strip()\n",
    "                else:\n",
    "                    course_info['course_rating'] = 'NaN'\n",
    "\n",
    "                review_value = rating_tag.find(\"p\", class_=\"css-vac8rf\")\n",
    "                if review_value:\n",
    "                    course_info['course_review'] = review_value.text.strip()\n",
    "                else:\n",
    "                    course_info['course_review'] = 'NaN'\n",
    "            else:\n",
    "                course_info['course_rating'] = 'NaN'\n",
    "                course_info['course_review'] = 'NaN'\n",
    "\n",
    "            # Append course info to list\n",
    "            course_data.append(course_info)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "    page_number += 1\n",
    "\n",
    "# Create directory if it does not exist\n",
    "output_directory = r'C:\\Users\\WCHuang8\\Desktop\\學習推薦系統專案\\coursera爬蟲\\所有課程資訊'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Write course data to a CSV file\n",
    "csv_filename = os.path.join(output_directory, f'course_data_AI.csv')\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['user_input', 'course_title', 'course_partner', 'course_review', 'skill_gain', 'course_rating', 'course_detail']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for course_info in course_data:\n",
    "        writer.writerow(course_info)\n",
    "\n",
    "print(\"Data extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_input                                       course_title  \\\n",
      "0         AI                         Generative AI in Marketing   \n",
      "1         AI                             AI Agents in LangGraph   \n",
      "2         AI  Post Graduate Certificate in Advanced Machine ...   \n",
      "3         AI  Post Graduate Certificate in Strategic Supply ...   \n",
      "4         AI                               Google AI Essentials   \n",
      "\n",
      "                                    course_partner   course_review  \\\n",
      "0  University of Virginia Darden School Foundation    (11 reviews)   \n",
      "1                                  DeepLearning.AI     (9 reviews)   \n",
      "2                                      IIT Roorkee             NaN   \n",
      "3                                      IIT Roorkee             NaN   \n",
      "4                                           Google  (1.3K reviews)   \n",
      "\n",
      "                                          skill_gain  course_rating  \\\n",
      "0                                                NaN            3.9   \n",
      "1                                                NaN            4.8   \n",
      "2                                                NaN            NaN   \n",
      "3  Skills you'll gain: Machine Learning, Python P...            NaN   \n",
      "4  Skills you'll gain: Critical Thinking, Problem...            4.7   \n",
      "\n",
      "                                  course_detail  \n",
      "0    Beginner Â· Specialization Â· 1 - 3 Months  \n",
      "1  Intermediate Â· Project Â· Less Than 2 Hours  \n",
      "2       University Certificate Â· 6 - 12 Months  \n",
      "3       University Certificate Â· 6 - 12 Months  \n",
      "4            Beginner Â· Course Â· 1 - 3 Months  \n",
      "  user_input                                       course_title  \\\n",
      "0         AI                         Generative AI in Marketing   \n",
      "1         AI                             AI Agents in LangGraph   \n",
      "2         AI  Post Graduate Certificate in Advanced Machine ...   \n",
      "3         AI  Post Graduate Certificate in Strategic Supply ...   \n",
      "4         AI                               Google AI Essentials   \n",
      "\n",
      "                                    course_partner   course_review  \\\n",
      "0  University of Virginia Darden School Foundation    (11 reviews)   \n",
      "1                                  DeepLearning.AI     (9 reviews)   \n",
      "2                                      IIT Roorkee             NaN   \n",
      "3                                      IIT Roorkee             NaN   \n",
      "4                                           Google  (1.3K reviews)   \n",
      "\n",
      "                                          skill_gain  course_rating  \\\n",
      "0                                                NaN            3.9   \n",
      "1                                                NaN            4.8   \n",
      "2                                                NaN            NaN   \n",
      "3  Skills you'll gain: Machine Learning, Python P...            NaN   \n",
      "4  Skills you'll gain: Critical Thinking, Problem...            4.7   \n",
      "\n",
      "                                course_detail  \n",
      "0    Beginner · Specialization · 1 - 3 Months  \n",
      "1  Intermediate · Project · Less Than 2 Hours  \n",
      "2      University Certificate · 6 - 12 Months  \n",
      "3      University Certificate · 6 - 12 Months  \n",
      "4            Beginner · Course · 1 - 3 Months  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WCHuang8\\AppData\\Local\\Temp\\ipykernel_1536\\555333789.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_data_science = df_data_science.applymap(lambda x: x.replace('Â', '') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取爬取到的數據\n",
    "df_data_science = pd.read_csv(\"C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_data_AI.csv\")\n",
    "\n",
    "# 查看數據的前幾行\n",
    "print(df_data_science.head())\n",
    "\n",
    "# 替換特殊字符\n",
    "df_data_science = df_data_science.applymap(lambda x: x.replace('Â', '') if isinstance(x, str) else x)\n",
    "\n",
    "# 查看清理後的數據的前幾行\n",
    "print(df_data_science.head())\n",
    "\n",
    "# 將數據保存為Excel檔案\n",
    "df_data_science.to_excel('C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_data_AI.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理後的數據：\n",
      "    user_input                                       course_title  \\\n",
      "4           AI                               Google AI Essentials   \n",
      "5           AI                                   IBM AI Developer   \n",
      "6           AI        Introduction to Generative AI Learning Path   \n",
      "7           AI                    IBM AI Foundations for Business   \n",
      "8           AI           Generative AI with Large Language Models   \n",
      "..         ...                                                ...   \n",
      "988         AI            Data Analysis with Spreadsheets and SQL   \n",
      "990         AI  Project Management Foundations, Initiation, an...   \n",
      "991         AI                  International Business Essentials   \n",
      "993         AI             Leading transformations: Manage change   \n",
      "994         AI                            Becoming a Sports Agent   \n",
      "\n",
      "                      course_partner   course_review  \\\n",
      "4                             Google  (1.3K reviews)   \n",
      "5                                IBM   (51K reviews)   \n",
      "6                       Google Cloud  (5.2K reviews)   \n",
      "7                                IBM   (82K reviews)   \n",
      "8                    DeepLearning.AI  (2.4K reviews)   \n",
      "..                               ...             ...   \n",
      "988                             Meta    (56 reviews)   \n",
      "990                   SkillUp EdTech   (210 reviews)   \n",
      "991             University of London  (1.8K reviews)   \n",
      "993             Macquarie University  (1.4K reviews)   \n",
      "994  Case Western Reserve University   (144 reviews)   \n",
      "\n",
      "                                            skill_gain  course_rating  \\\n",
      "4    Skills you'll gain: Critical Thinking, Problem...            4.7   \n",
      "5    Skills you'll gain: Machine Learning, Deep Lea...            4.6   \n",
      "6    Skills you'll gain: Artificial Neural Networks...            4.6   \n",
      "7    Skills you'll gain: Machine Learning, Data Ana...            4.7   \n",
      "8    Skills you'll gain: Machine Learning, Natural ...            4.8   \n",
      "..                                                 ...            ...   \n",
      "988  Skills you'll gain: Data Analysis, Data Visual...            4.5   \n",
      "990  Skills you'll gain: Leadership and Management,...            4.6   \n",
      "991  Skills you'll gain: Mathematical Theory & Anal...            4.6   \n",
      "993  Skills you'll gain: Change Management, Leaders...            4.7   \n",
      "994  Skills you'll gain: Marketing, Negotiation, Pe...            4.7   \n",
      "\n",
      "                                         course_detail  \n",
      "4                     Beginner · Course · 1 - 3 Months  \n",
      "5    Beginner · Professional Certificate · 3 - 6 Mo...  \n",
      "6         Intermediate · Specialization · 1 - 3 Months  \n",
      "7             Beginner · Specialization · 1 - 3 Months  \n",
      "8                  Intermediate · Course · 1 - 4 Weeks  \n",
      "..                                                 ...  \n",
      "988                   Beginner · Course · 1 - 3 Months  \n",
      "990                   Beginner · Course · 1 - 3 Months  \n",
      "991       Intermediate · Specialization · 3 - 6 Months  \n",
      "993                   Beginner · Course · 1 - 3 Months  \n",
      "994                   Beginner · Course · 1 - 3 Months  \n",
      "\n",
      "[636 rows x 7 columns]\n",
      "清理完成，結果已保存到新的Excel文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取Excel文件\n",
    "file_path = 'C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_data_AI.xlsx'  # 這是上傳的文件路徑\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 刪除包含缺漏值（NaN）的行\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# 檢查清理後的數據\n",
    "print(\"清理後的數據：\")\n",
    "print(df_cleaned)\n",
    "\n",
    "# 將清理後的數據保存到新的Excel文件\n",
    "output_file_path = 'C:/Users/WCHuang8/Desktop/學習推薦系統專案/coursera爬蟲/所有課程資訊/course_data_AI_cleaned.xlsx'\n",
    "df_cleaned.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"清理完成，結果已保存到新的Excel文件。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
